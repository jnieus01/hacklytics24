{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6070d-00ea-4f92-bc70-b4f0a9720c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "!{sys.executable} -m pip install langchain==0.0.335 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install pygpt4all==1.1.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install gpt4all==1.0.12 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install transformers==4.35.1 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install datasets==2.14.6 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install tiktoken==0.4.0 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install chromadb==0.4.15 --no-warn-script-location > /dev/null\n",
    "!{sys.executable} -m pip install sentence_transformers==2.2.2 --no-warn-script-location > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f494fb-2d79-41f4-ae36-ccb6445b6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class RAGBot:\n",
    "    \"\"\"\n",
    "    A class to handle model downloading, dataset management, model loading, vector database\n",
    "    creation, retrieval mechanisms, and inference for a response generation bot.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model_path : str\n",
    "        The file path where the model is stored.\n",
    "    data_path : str\n",
    "        The file path where the dataset is stored.\n",
    "    user_input : str\n",
    "        The input provided by the user for generating a response.\n",
    "    model : str\n",
    "        The name of the model being used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the RAGBot with default values for model path, data path,\n",
    "        user input, and model.\n",
    "        \"\"\"\n",
    "        self.model_path = \"\"\n",
    "        self.data_path = \"\"\n",
    "        self.user_input = \"\"\n",
    "        self.model = \"\"\n",
    "\n",
    "    def get_model(self, model, chunk_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Downloads the specified model to the model path. Supports downloading of large\n",
    "        models in chunks.\n",
    "\n",
    "        Additional download tooling is reserved for users to add their own models. Currently hardcoded to load Falcon from \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : str\n",
    "            The name of the model to be downloaded.\n",
    "        chunk_size : int, optional\n",
    "            The size of each chunk of data to download at a time, by default 10000.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        if self.model == \"Falcon\":\n",
    "            self.model_path = \"/home/common/data/Big_Data/GenAI/llm_models/nomic-ai--gpt4all-falcon-ggml/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "        elif model == \"More Models Coming Soon!\":\n",
    "            print(\"More models coming soon, defaulting to Falcon for now!\")\n",
    "            self.model_path = \"/home/common/data/Big_Data/GenAI/llm_models/nomic-ai--gpt4all-falcon-ggml/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "\n",
    "        if not os.path.isfile(self.model_path):\n",
    "            # send a GET request to the URL to download the file. Stream since it's large\n",
    "            response = requests.get(url, stream=True)\n",
    "            # open the file in binary mode and write the contents of the response to it in chunks\n",
    "            # This is a large file, so be prepared to wait.\n",
    "            with open(self.model_path, 'wb') as f:\n",
    "                for chunk in tqdm(response.iter_content(chunk_size=10000)):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        else:\n",
    "            print('model already exists in path.')\n",
    "\n",
    "    # def download_dataset(self, dataset):\n",
    "    #     \"\"\"\n",
    "    #     Downloads the specified dataset and saves it to the data path.\n",
    "\n",
    "    #     Parameters\n",
    "    #     ----------\n",
    "    #     dataset : str\n",
    "    #         The name of the dataset to be downloaded.\n",
    "    #     \"\"\"\n",
    "    #     self.data_path = dataset + '_dialogues.txt'\n",
    "\n",
    "    #     if not os.path.isfile(self.data_path):\n",
    "\n",
    "    #         datasets = {\"robot maintenance\": \"FunDialogues/customer-service-robot-support\", \n",
    "    #                     \"basketball coach\": \"FunDialogues/sports-basketball-coach\", \n",
    "    #                     \"physics professor\": \"FunDialogues/academia-physics-office-hours\",\n",
    "    #                     \"grocery cashier\" : \"FunDialogues/customer-service-grocery-cashier\"}\n",
    "            \n",
    "    #         # Download the dialogue from hugging face\n",
    "    #         dataset = load_dataset(f\"{datasets[dataset]}\")\n",
    "    #         # Convert the dataset to a pandas dataframe\n",
    "    #         dialogues = dataset['train']\n",
    "    #         df = pd.DataFrame(dialogues, columns=['id', 'description', 'dialogue'])\n",
    "    #         # Print the first 5 rows of the dataframe\n",
    "    #         df.head()\n",
    "    #         # only keep the dialogue column\n",
    "    #         dialog_df = df['dialogue']\n",
    "            \n",
    "    #         # save the data to txt file\n",
    "    #         dialog_df.to_csv(self.data_path, sep=' ', index=False)\n",
    "    #     else:\n",
    "    #         print('data already exists in path.')        \n",
    "    def set_data_path(self):\n",
    "        \"\"\"\n",
    "        Sets the data path to the preprocessed hotel dataset.\n",
    "        \"\"\"\n",
    "        self.data_path = 'hotel_data_for_vector_db.txt'\n",
    "\n",
    "\n",
    "    def load_model(self, n_threads, max_tokens, repeat_penalty, n_batch, top_k, temp):\n",
    "        \"\"\"\n",
    "        Loads the model with specified parameters for parallel processing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_threads : int\n",
    "            The number of threads for parallel processing.\n",
    "        max_tokens : int\n",
    "            The maximum number of tokens for model prediction.\n",
    "        repeat_penalty : float\n",
    "            The penalty for repeated tokens in generation.\n",
    "        n_batch : int\n",
    "            The number of batches for processing.\n",
    "        top_k : int\n",
    "            The number of top k tokens to be considered in sampling.\n",
    "        \"\"\"\n",
    "        # Callbacks support token-wise streaming\n",
    "        callbacks = [StreamingStdOutCallbackHandler()]\n",
    "        # Verbose is required to pass to the callback manager\n",
    "\n",
    "        self.llm = GPT4All(model=self.model_path, callbacks=callbacks, verbose=False,\n",
    "                           n_threads=n_threads, n_predict=max_tokens, repeat_penalty=repeat_penalty, \n",
    "                           n_batch=n_batch, top_k=top_k, temp=temp)\n",
    "\n",
    "    def build_vectordb(self, chunk_size, overlap):\n",
    "        \"\"\"\n",
    "        Builds a vector database from the dataset for retrieval purposes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        chunk_size : int\n",
    "            The size of text chunks for vectorization.\n",
    "        overlap : int\n",
    "            The overlap size between chunks.\n",
    "        \"\"\"\n",
    "        loader = TextLoader(self.data_path)\n",
    "        # Text Splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "        # Embed the document and store into chroma DB\n",
    "        self.index = VectorstoreIndexCreator(embedding= HuggingFaceEmbeddings(), text_splitter=text_splitter).from_loaders([loader])\n",
    "\n",
    "    def retrieval_mechanism(self, user_input, top_k=1, context_verbosity = False, rag_off= False):\n",
    "        \"\"\"\n",
    "        Retrieves relevant document snippets based on the user's query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_input : str\n",
    "            The user's input or query.\n",
    "        top_k : int, optional\n",
    "            The number of top results to return, by default 1.\n",
    "        context_verbosity : bool, optional\n",
    "            If True, additional context information is printed, by default False.\n",
    "        rag_off : bool, optional\n",
    "            If True, disables the retrieval-augmented generation, by default False.\n",
    "        \"\"\"\n",
    "\n",
    "        self.user_input = user_input\n",
    "        self.context_verbosity = context_verbosity\n",
    "                \n",
    "        # perform a similarity search and retrieve the context from our documents\n",
    "        results = self.index.vectorstore.similarity_search(self.user_input, k=top_k)\n",
    "        # join all context information into one string \n",
    "        context = \"\\n\".join([document.page_content for document in results])\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Retrieving information related to your question...\")\n",
    "            print(f\"Found this content which is most similar to your question: {context}\")\n",
    "\n",
    "        if rag_off:\n",
    "            template = \"\"\"Question: {question}\n",
    "            Answer: This is the response: \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        else:     \n",
    "            template = \"\"\" Don't just repeat the following context, use it in combination with your knowledge to improve your answer to the question:{context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Performs inference to generate a response based on the user's query.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The generated response.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Your Query: {self.prompt}\")\n",
    "            \n",
    "        llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
    "        print(\"Processing the information with gpt4all...\\n\")\n",
    "        response = llm_chain.run(self.user_input)\n",
    "\n",
    "        return  response  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9501f49-a18b-414e-9e00-4065c63f8258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768cd93be8eb4e77828e834fb5928a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Model:', options=('Falcon', 'More Models Coming Soon!'), vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29adf300918247218d260b478e104f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "bot = RAGBot()\n",
    "\n",
    "# Initialize previous value variables\n",
    "previous_threads = None\n",
    "previous_max_tokens = None\n",
    "previous_top_k = None\n",
    "previous_dataset = None\n",
    "previous_chunk_size = None\n",
    "previous_overlap = None\n",
    "previous_temp = None\n",
    "\n",
    "# Create an output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_inputs(b):\n",
    "    \"\"\"\n",
    "    Process inputs from the interactive chat interface.\n",
    "\n",
    "    This function is triggered by a button click in the IPython widgets interface. It captures \n",
    "    user inputs from various widget elements, such as dropdowns, sliders, and text inputs. The function \n",
    "    handles model and dataset downloading, initiates model loading and vector database building, \n",
    "    performs the retrieval mechanism, and generates a response to the user's query. The response is \n",
    "    then displayed in a styled HTML format within the Jupyter Notebook.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b : ipywidgets.widgets.widget_button.Button\n",
    "        The button widget that triggers this function. This parameter is required by the\n",
    "        widget framework but is not directly used in the function.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function is designed to be used as a callback for an IPython button widget.\n",
    "    - It utilizes global variables to access and update the widget elements and their values.\n",
    "    - The function updates global variables to keep track of previous parameter values for \n",
    "      efficient reloading of models and rebuilding of vector databases.\n",
    "    - Standard output and error output are captured and redirected to suppress unnecessary console logs,\n",
    "      while relevant output is displayed via the IPython display mechanism.\n",
    "    \"\"\"\n",
    "    global previous_threads, previous_max_tokens, previous_top_k, previous_dataset, previous_chunk_size, previous_overlap, previous_temp\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        # Suppress output\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "\n",
    "            # Function to process inputs\n",
    "            # Gather values from the widgets\n",
    "            model = model_dropdown.value\n",
    "            query = query_text.value\n",
    "            top_k = top_k_slider.value\n",
    "            chunk_size = chunk_size_input.value\n",
    "            overlap = overlap_input.value\n",
    "            dataset = dataset_dropdown.value\n",
    "            threads = threads_slider.value\n",
    "            max_tokens = max_token_input.value\n",
    "            rag_off = rag_off_checkbox.value\n",
    "            temp = temp_slider.value\n",
    "            bot.get_model(model = model)\n",
    "            bot.set_data_path()\n",
    "            # bot.download_dataset(dataset = dataset)\n",
    "            if threads != previous_threads or max_tokens != previous_max_tokens or top_k != previous_top_k or temp != previous_temp:\n",
    "                print(\"loading model due incorporate new parameters\")\n",
    "                bot.load_model(n_threads=threads, max_tokens=max_tokens, repeat_penalty=1.50, n_batch=threads, top_k=top_k, temp=temp)\n",
    "                # Update previous values\n",
    "                previous_threads = threads\n",
    "                previous_max_tokens = max_tokens\n",
    "                previous_top_k = top_k\n",
    "                previous_temp = temp\n",
    "            if dataset != previous_dataset or chunk_size != previous_chunk_size or overlap != previous_overlap:\n",
    "                print(\"rebuilding vector DB due to changing dataset, overlap, or chunk\")\n",
    "                bot.build_vectordb(chunk_size = chunk_size, overlap = overlap)\n",
    "                previous_dataset = dataset\n",
    "                previous_chunk_size = chunk_size\n",
    "                previous_overlap = overlap\n",
    "            bot.retrieval_mechanism(user_input = query, rag_off = rag_off)\n",
    "            response = bot.inference()\n",
    "    \n",
    "            styled_response = f\"\"\"\n",
    "            <div style=\"\n",
    "                background-color: lightblue;\n",
    "                border-radius: 15px;\n",
    "                padding: 10px;\n",
    "                font-family: Arial, sans-serif;\n",
    "                color: black;\n",
    "                max-width: 600px;\n",
    "                word-wrap: break-word;\n",
    "                margin: 10px;\n",
    "                font-size: 14px;\">\n",
    "                {response}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            display(HTML(styled_response))\n",
    "\n",
    "def create_chat_interface():\n",
    "    global model_dropdown, query_text, top_k_slider, rag_off_checkbox, chunk_size_input, overlap_input, dataset_dropdown, threads_slider, max_token_input, repeat_penalty_input, temp_slider\n",
    "    # Model selection dropdown\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['Falcon', 'More Models Coming Soon!'],\n",
    "        description='Model:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # User query text input\n",
    "    query_layout = widgets.Layout(width='400px', height='400px')  # Adjust the width as needed\n",
    "    query_text = widgets.Text(\n",
    "        placeholder='Type your query here',\n",
    "        description='Query:',\n",
    "        disabled=False, \n",
    "        layout=query_layout\n",
    "    )\n",
    "\n",
    "    # Vector search top k slider\n",
    "    top_k_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=4,\n",
    "        step=1,\n",
    "        description='Top K:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    # Model Temperature slider\n",
    "    temp_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=1.4,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f'\n",
    ")\n",
    "    \n",
    "    # RAG OFF TOGGLE\n",
    "    rag_off_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='RAG OFF?',\n",
    "    disabled=False,\n",
    "    indent=False,  # Set to True if you want the checkbox to be indented\n",
    "    tooltip='Turns off RAG and Performs Inference with Raw Model and Prompt Only'\n",
    "    )\n",
    "\n",
    "    # Chunk size number input\n",
    "    chunk_size_input = widgets.BoundedIntText(\n",
    "        value=500,\n",
    "        min=5,\n",
    "        max=5000,\n",
    "        step=1,\n",
    "        description='Chunk Size:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Overlap number input\n",
    "    overlap_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=0,\n",
    "        max=1000,\n",
    "        step=1,\n",
    "        description='Overlap:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Dataset selection dropdown\n",
    "    dataset_dropdown = widgets.Dropdown(\n",
    "        options=['robot maintenance', 'basketball coach', 'physics professor', 'grocery cashier'],\n",
    "        description='Dataset:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Number of threads slider\n",
    "    threads_slider = widgets.IntSlider(\n",
    "        value=64,\n",
    "        min=2,\n",
    "        max=200,\n",
    "        step=1,\n",
    "        description='Threads:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    # Max token number input\n",
    "    max_token_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=5,\n",
    "        max=500,\n",
    "        step=5,\n",
    "        description='Max Tokens:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Group the widgets except the query text into a VBox\n",
    "    left_column = widgets.VBox([model_dropdown, top_k_slider, temp_slider, rag_off_checkbox, chunk_size_input, \n",
    "                                overlap_input, dataset_dropdown, threads_slider, max_token_input])\n",
    "\n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(process_inputs)\n",
    "\n",
    "    right_column = widgets.VBox([query_text, submit_button])\n",
    "\n",
    "    # Use HBox to position the VBox and query text side by side\n",
    "    interface_layout = widgets.HBox([left_column, right_column])\n",
    "\n",
    "\n",
    "    # Display the layout\n",
    "    display(interface_layout, output)\n",
    "\n",
    "create_chat_interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6712797-62fc-41aa-88f2-bf296374d029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
